{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1CNg5sUd4Yf8hO7VQo8hJuX61pzNog_P6",
      "authorship_tag": "ABX9TyNNV2FneRANT07+bzssFgBx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thaianh1210/DeepLearning-exercise/blob/main/Untitled49.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NURXZT8fk6KX"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65U3CtekI3j0",
        "outputId": "7a878bde-b98c-4005-bade-ccbc3cc0488c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = '/content/drive/MyDrive/img_cls_weather_dataset/weather-dataset/dataset'\n",
        "img_paths = []\n",
        "labels = []\n",
        "classes = {\n",
        "    label_idx: class_name \\\n",
        "    for label_idx, class_name in enumerate(sorted(os.listdir(root_dir)))\n",
        "}\n",
        "\n",
        "print(classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDgN69rYwvzf",
        "outputId": "fc258c17-831d-414c-84cc-0c8f5734b728"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'dew', 1: 'fogsmog', 2: 'frost', 3: 'glaze', 4: 'hail', 5: 'lightning', 6: 'rain', 7: 'rainbow', 8: 'rime', 9: 'sandstorm', 10: 'snow'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for label_idx, class_name in classes.items():\n",
        "  class_dir = os.path.join(root_dir, class_name)\n",
        "  for img_filename in os.listdir(class_dir):\n",
        "    img_path = os.path.join(class_dir, img_filename)\n",
        "    img_paths.append(img_path)\n",
        "    labels.append(label_idx)\n",
        "print(img_paths[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxbUjjgkx_-D",
        "outputId": "a65b5006-beac-4efa-eb5a-bdeac9bd8801"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/img_cls_weather_dataset/weather-dataset/dataset/snow/0830.jpg', '/content/drive/MyDrive/img_cls_weather_dataset/weather-dataset/dataset/snow/0834.jpg', '/content/drive/MyDrive/img_cls_weather_dataset/weather-dataset/dataset/snow/0835.jpg', '/content/drive/MyDrive/img_cls_weather_dataset/weather-dataset/dataset/snow/0846.jpg', '/content/drive/MyDrive/img_cls_weather_dataset/weather-dataset/dataset/snow/0838.jpg', '/content/drive/MyDrive/img_cls_weather_dataset/weather-dataset/dataset/snow/0844.jpg', '/content/drive/MyDrive/img_cls_weather_dataset/weather-dataset/dataset/snow/0845.jpg', '/content/drive/MyDrive/img_cls_weather_dataset/weather-dataset/dataset/snow/0840.jpg', '/content/drive/MyDrive/img_cls_weather_dataset/weather-dataset/dataset/snow/0843.jpg', '/content/drive/MyDrive/img_cls_weather_dataset/weather-dataset/dataset/snow/0831.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "val_size = 0.2\n",
        "test_size = 0.1\n",
        "is_shuffle = True\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    img_paths, labels, test_size = test_size, random_state = seed, shuffle = is_shuffle\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size = val_size, random_state = seed, shuffle = is_shuffle\n",
        ")\n"
      ],
      "metadata": {
        "id": "b79NxqWjywl-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeatherDataset(Dataset):\n",
        "  def __init__(self, X, y, transform = None):\n",
        "    self.transform = transform\n",
        "    self.img_paths = X\n",
        "    self.labels = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_paths)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.img_paths[idx]\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    return img, self.labels[idx]\n",
        "\n",
        "def transform(img, img_size = (224, 224)):\n",
        "    img = img.resize(img_size)\n",
        "    img =  np.array(img)[..., :3]\n",
        "    img = torch.tensor(img).permute(2, 0, 1).float()\n",
        "    normalize_img = img/ 255.0\n",
        "    return normalize_img"
      ],
      "metadata": {
        "id": "fceMo2ZG040A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = WeatherDataset(X_train, y_train, transform = transform)\n",
        "val_dataset = WeatherDataset(X_val, y_val, transform = transform)\n",
        "test_dataset = WeatherDataset(X_test, y_test, transform = transform)"
      ],
      "metadata": {
        "id": "cwiGRYKl3wjE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 512\n",
        "test_batch_size = 8\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size = test_batch_size, shuffle = False)\n",
        "test_loaderr = DataLoader(test_dataset, batch_size = test_batch_size, shuffle = False)"
      ],
      "metadata": {
        "id": "oXYgOIgz9jjY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create ResNet for training\n",
        "#First, create a Class of Residual Block\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride = 1):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1)\n",
        "    self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
        "    self.downsample = nn.Sequential()\n",
        "\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      self.downsample = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = stride),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x.clone()\n",
        "    x = self.conv1(x)\n",
        "    x = self.batch_norm1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.batch_norm2(x)\n",
        "    x += self.downsample(shortcut)\n",
        "    x = self.relu(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "sBmuZfF3JT_K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, residual_block, n_block_lst, n_classes):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3)\n",
        "    self.batch_norm1 = nn.BatchNorm2d(64)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "    self.conv2 = self.create_layer(residual_block, 64, 64, n_block_lst[0], 1)\n",
        "    self.conv3 = self.create_layer(residual_block, 64, 128, n_block_lst[1], 2)\n",
        "    self.conv4 = self.create_layer(residual_block, 128, 256, n_block_lst[2], 2)\n",
        "    self.conv5 = self.create_layer(residual_block, 256, 512, n_block_lst[3], 2)\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.fc = nn.Linear(512, n_classes)\n",
        "    self. flatten = nn.Flatten()\n",
        "\n",
        "    # Correct the indentation of create_layer function\n",
        "  def create_layer(self, residual_block, in_channels, out_channels, n_blocks, stride):\n",
        "      blocks = []\n",
        "      first_block = residual_block(in_channels, out_channels, stride)\n",
        "      blocks.append(first_block)\n",
        "\n",
        "      for idx in range(1, n_blocks):\n",
        "        block = residual_block(out_channels, out_channels, stride)\n",
        "        blocks.append(block)\n",
        "      return nn.Sequential(*blocks)\n",
        "\n",
        "    # Correct the indentation of forward function\n",
        "  def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.batch_norm1(x) # Fix typo: batchnorm1 -> batch_norm1\n",
        "      x = self.maxpool(x) # Fix typo: maxpool1 -> maxpool\n",
        "      x = self.relu(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.conv3(x)\n",
        "      x = self.conv4(x)\n",
        "      x = self.conv5(x)\n",
        "      x = self.avgpool(x)\n",
        "      x = self.flatten(x)\n",
        "      x = self.fc(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "DUUOMLavkEMs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(list(classes.keys()))\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = ResNet(residual_block = ResidualBlock, n_block_lst = [2, 2, 2, 2], n_classes = n_classes).to(device)"
      ],
      "metadata": {
        "id": "k2jXHTSEfmHO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_loader, criterion, device):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  losses = []\n",
        "  with torch.no_grad():\n",
        "    for images, labels, in val_loader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      y_hat = model(images)\n",
        "      loss = criterion(y_hat, labels)\n",
        "      losses.append(loss.item())\n",
        "      _, predicted = torch.max(y_hat.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  loss = sum(losses) / len(losses)\n",
        "  acc = correct / total\n",
        "  return loss, acc"
      ],
      "metadata": {
        "id": "pXMwfIIWkiX-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  for epoch in range(epochs):\n",
        "    batch_train_losses = []\n",
        "    model.train()\n",
        "    for idx, (inputs, labels) in enumerate(train_loader):\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      batch_train_losses.append(loss)\n",
        "    train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
        "    train_losses.append(train_loss)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "  return train_losses, val_losses\n"
      ],
      "metadata": {
        "id": "51nN2__xpGmo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = lr)"
      ],
      "metadata": {
        "id": "WTzH0Dnvt9XV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses = fit(model, train_loader, val_loader, criterion, optimizer, device, epochs = 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldTWGwGvz6se",
        "outputId": "dc2a06c5-5dc7-4cd1-cfd9-ce7561399a59"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Train Loss: 2.6542, Val Loss: 2.4223, Val Acc: 0.0000\n",
            "Epoch 2/200, Train Loss: 1.7605, Val Loss: 2.4218, Val Acc: 0.0000\n",
            "Epoch 3/200, Train Loss: 1.4780, Val Loss: 2.4143, Val Acc: 0.0000\n",
            "Epoch 4/200, Train Loss: 1.0839, Val Loss: 2.4062, Val Acc: 0.0000\n",
            "Epoch 5/200, Train Loss: 0.8230, Val Loss: 2.3908, Val Acc: 0.0000\n",
            "Epoch 6/200, Train Loss: 0.6968, Val Loss: 2.3732, Val Acc: 0.0000\n",
            "Epoch 7/200, Train Loss: 0.6225, Val Loss: 2.3542, Val Acc: 0.0000\n",
            "Epoch 8/200, Train Loss: 0.4749, Val Loss: 2.3336, Val Acc: 0.0000\n",
            "Epoch 9/200, Train Loss: 0.4163, Val Loss: 2.3098, Val Acc: 0.0000\n",
            "Epoch 10/200, Train Loss: 0.2957, Val Loss: 2.2859, Val Acc: 1.0000\n",
            "Epoch 11/200, Train Loss: 0.2441, Val Loss: 2.2613, Val Acc: 1.0000\n",
            "Epoch 12/200, Train Loss: 0.2010, Val Loss: 2.2378, Val Acc: 1.0000\n",
            "Epoch 13/200, Train Loss: 0.1806, Val Loss: 2.2136, Val Acc: 1.0000\n",
            "Epoch 14/200, Train Loss: 0.1640, Val Loss: 2.1945, Val Acc: 1.0000\n",
            "Epoch 15/200, Train Loss: 0.1448, Val Loss: 2.1730, Val Acc: 1.0000\n",
            "Epoch 16/200, Train Loss: 0.1304, Val Loss: 2.1541, Val Acc: 1.0000\n",
            "Epoch 17/200, Train Loss: 0.1190, Val Loss: 2.1352, Val Acc: 1.0000\n",
            "Epoch 18/200, Train Loss: 0.1119, Val Loss: 2.1164, Val Acc: 1.0000\n",
            "Epoch 19/200, Train Loss: 0.1085, Val Loss: 2.0964, Val Acc: 1.0000\n",
            "Epoch 20/200, Train Loss: 0.1065, Val Loss: 2.0746, Val Acc: 1.0000\n",
            "Epoch 21/200, Train Loss: 0.1010, Val Loss: 2.0547, Val Acc: 1.0000\n",
            "Epoch 22/200, Train Loss: 0.0984, Val Loss: 2.0346, Val Acc: 1.0000\n",
            "Epoch 23/200, Train Loss: 0.0909, Val Loss: 2.0199, Val Acc: 1.0000\n",
            "Epoch 24/200, Train Loss: 0.0858, Val Loss: 1.9955, Val Acc: 0.7500\n",
            "Epoch 25/200, Train Loss: 0.0818, Val Loss: 1.9769, Val Acc: 0.7500\n",
            "Epoch 26/200, Train Loss: 0.0787, Val Loss: 1.9444, Val Acc: 1.0000\n",
            "Epoch 27/200, Train Loss: 0.0756, Val Loss: 1.9238, Val Acc: 1.0000\n",
            "Epoch 28/200, Train Loss: 0.0722, Val Loss: 1.9075, Val Acc: 1.0000\n",
            "Epoch 29/200, Train Loss: 0.0692, Val Loss: 1.8955, Val Acc: 0.7500\n",
            "Epoch 30/200, Train Loss: 0.0663, Val Loss: 1.8622, Val Acc: 0.7500\n",
            "Epoch 31/200, Train Loss: 0.0643, Val Loss: 1.8286, Val Acc: 1.0000\n",
            "Epoch 32/200, Train Loss: 0.0633, Val Loss: 1.8012, Val Acc: 1.0000\n",
            "Epoch 33/200, Train Loss: 0.0624, Val Loss: 1.7855, Val Acc: 1.0000\n",
            "Epoch 34/200, Train Loss: 0.0616, Val Loss: 1.7674, Val Acc: 1.0000\n",
            "Epoch 35/200, Train Loss: 0.0599, Val Loss: 1.7244, Val Acc: 1.0000\n",
            "Epoch 36/200, Train Loss: 0.0578, Val Loss: 1.7100, Val Acc: 1.0000\n",
            "Epoch 37/200, Train Loss: 0.0567, Val Loss: 1.6652, Val Acc: 1.0000\n",
            "Epoch 38/200, Train Loss: 0.0559, Val Loss: 1.6257, Val Acc: 1.0000\n",
            "Epoch 39/200, Train Loss: 0.0546, Val Loss: 1.5978, Val Acc: 1.0000\n",
            "Epoch 40/200, Train Loss: 0.0529, Val Loss: 1.5664, Val Acc: 1.0000\n",
            "Epoch 41/200, Train Loss: 0.0511, Val Loss: 1.5732, Val Acc: 1.0000\n",
            "Epoch 42/200, Train Loss: 0.0499, Val Loss: 1.5183, Val Acc: 1.0000\n",
            "Epoch 43/200, Train Loss: 0.0487, Val Loss: 1.4787, Val Acc: 1.0000\n",
            "Epoch 44/200, Train Loss: 0.0474, Val Loss: 1.4078, Val Acc: 1.0000\n",
            "Epoch 45/200, Train Loss: 0.0466, Val Loss: 1.3843, Val Acc: 1.0000\n",
            "Epoch 46/200, Train Loss: 0.0458, Val Loss: 1.3846, Val Acc: 1.0000\n",
            "Epoch 47/200, Train Loss: 0.0452, Val Loss: 1.3937, Val Acc: 1.0000\n",
            "Epoch 48/200, Train Loss: 0.0443, Val Loss: 1.3757, Val Acc: 1.0000\n",
            "Epoch 49/200, Train Loss: 0.0438, Val Loss: 1.3618, Val Acc: 1.0000\n",
            "Epoch 50/200, Train Loss: 0.0429, Val Loss: 1.3656, Val Acc: 1.0000\n",
            "Epoch 51/200, Train Loss: 0.0422, Val Loss: 1.3756, Val Acc: 1.0000\n",
            "Epoch 52/200, Train Loss: 0.0414, Val Loss: 1.3439, Val Acc: 1.0000\n",
            "Epoch 53/200, Train Loss: 0.0408, Val Loss: 1.3732, Val Acc: 1.0000\n",
            "Epoch 54/200, Train Loss: 0.0399, Val Loss: 1.3381, Val Acc: 1.0000\n",
            "Epoch 55/200, Train Loss: 0.0394, Val Loss: 1.3472, Val Acc: 1.0000\n",
            "Epoch 56/200, Train Loss: 0.0390, Val Loss: 1.3355, Val Acc: 1.0000\n",
            "Epoch 57/200, Train Loss: 0.0382, Val Loss: 1.3215, Val Acc: 1.0000\n",
            "Epoch 58/200, Train Loss: 0.0376, Val Loss: 1.3180, Val Acc: 0.7500\n",
            "Epoch 59/200, Train Loss: 0.0370, Val Loss: 1.2969, Val Acc: 0.7500\n",
            "Epoch 60/200, Train Loss: 0.0368, Val Loss: 1.2814, Val Acc: 0.7500\n",
            "Epoch 61/200, Train Loss: 0.0362, Val Loss: 1.2774, Val Acc: 0.7500\n",
            "Epoch 62/200, Train Loss: 0.0358, Val Loss: 1.2482, Val Acc: 0.7500\n",
            "Epoch 63/200, Train Loss: 0.0352, Val Loss: 1.2601, Val Acc: 0.7500\n",
            "Epoch 64/200, Train Loss: 0.0348, Val Loss: 1.2252, Val Acc: 0.7500\n",
            "Epoch 65/200, Train Loss: 0.0344, Val Loss: 1.2516, Val Acc: 0.7500\n",
            "Epoch 66/200, Train Loss: 0.0338, Val Loss: 1.2259, Val Acc: 0.7500\n",
            "Epoch 67/200, Train Loss: 0.0333, Val Loss: 1.2487, Val Acc: 0.7500\n",
            "Epoch 68/200, Train Loss: 0.0328, Val Loss: 1.2261, Val Acc: 0.7500\n",
            "Epoch 69/200, Train Loss: 0.0324, Val Loss: 1.2361, Val Acc: 0.7500\n",
            "Epoch 70/200, Train Loss: 0.0319, Val Loss: 1.2377, Val Acc: 0.7500\n",
            "Epoch 71/200, Train Loss: 0.0315, Val Loss: 1.2151, Val Acc: 0.7500\n",
            "Epoch 72/200, Train Loss: 0.0313, Val Loss: 1.2555, Val Acc: 0.7500\n",
            "Epoch 73/200, Train Loss: 0.0311, Val Loss: 1.1973, Val Acc: 0.7500\n",
            "Epoch 74/200, Train Loss: 0.0307, Val Loss: 1.2549, Val Acc: 0.7500\n",
            "Epoch 75/200, Train Loss: 0.0302, Val Loss: 1.1818, Val Acc: 0.7500\n",
            "Epoch 76/200, Train Loss: 0.0298, Val Loss: 1.2355, Val Acc: 0.7500\n",
            "Epoch 77/200, Train Loss: 0.0294, Val Loss: 1.1641, Val Acc: 0.7500\n",
            "Epoch 78/200, Train Loss: 0.0291, Val Loss: 1.2261, Val Acc: 0.7500\n",
            "Epoch 79/200, Train Loss: 0.0289, Val Loss: 1.1527, Val Acc: 0.7500\n",
            "Epoch 80/200, Train Loss: 0.0286, Val Loss: 1.2108, Val Acc: 0.7500\n",
            "Epoch 81/200, Train Loss: 0.0283, Val Loss: 1.1518, Val Acc: 0.7500\n",
            "Epoch 82/200, Train Loss: 0.0280, Val Loss: 1.1945, Val Acc: 0.7500\n",
            "Epoch 83/200, Train Loss: 0.0275, Val Loss: 1.1582, Val Acc: 0.7500\n",
            "Epoch 84/200, Train Loss: 0.0273, Val Loss: 1.1774, Val Acc: 0.7500\n",
            "Epoch 85/200, Train Loss: 0.0271, Val Loss: 1.1585, Val Acc: 0.7500\n",
            "Epoch 86/200, Train Loss: 0.0267, Val Loss: 1.1557, Val Acc: 0.7500\n",
            "Epoch 87/200, Train Loss: 0.0265, Val Loss: 1.1702, Val Acc: 0.7500\n",
            "Epoch 88/200, Train Loss: 0.0262, Val Loss: 1.1436, Val Acc: 0.7500\n",
            "Epoch 89/200, Train Loss: 0.0260, Val Loss: 1.1689, Val Acc: 0.7500\n",
            "Epoch 90/200, Train Loss: 0.0257, Val Loss: 1.1327, Val Acc: 0.7500\n",
            "Epoch 91/200, Train Loss: 0.0256, Val Loss: 1.1600, Val Acc: 0.7500\n",
            "Epoch 92/200, Train Loss: 0.0255, Val Loss: 1.1408, Val Acc: 0.7500\n",
            "Epoch 93/200, Train Loss: 0.0252, Val Loss: 1.1434, Val Acc: 0.7500\n",
            "Epoch 94/200, Train Loss: 0.0250, Val Loss: 1.1518, Val Acc: 0.7500\n",
            "Epoch 95/200, Train Loss: 0.0247, Val Loss: 1.1327, Val Acc: 0.7500\n",
            "Epoch 96/200, Train Loss: 0.0245, Val Loss: 1.1536, Val Acc: 0.7500\n",
            "Epoch 97/200, Train Loss: 0.0242, Val Loss: 1.1205, Val Acc: 0.7500\n",
            "Epoch 98/200, Train Loss: 0.0240, Val Loss: 1.1528, Val Acc: 0.7500\n",
            "Epoch 99/200, Train Loss: 0.0238, Val Loss: 1.1100, Val Acc: 0.7500\n",
            "Epoch 100/200, Train Loss: 0.0236, Val Loss: 1.1438, Val Acc: 0.7500\n",
            "Epoch 101/200, Train Loss: 0.0234, Val Loss: 1.1050, Val Acc: 1.0000\n",
            "Epoch 102/200, Train Loss: 0.0232, Val Loss: 1.1276, Val Acc: 0.7500\n",
            "Epoch 103/200, Train Loss: 0.0229, Val Loss: 1.0955, Val Acc: 1.0000\n",
            "Epoch 104/200, Train Loss: 0.0228, Val Loss: 1.1239, Val Acc: 0.7500\n",
            "Epoch 105/200, Train Loss: 0.0226, Val Loss: 1.0945, Val Acc: 1.0000\n",
            "Epoch 106/200, Train Loss: 0.0224, Val Loss: 1.1101, Val Acc: 0.7500\n",
            "Epoch 107/200, Train Loss: 0.0221, Val Loss: 1.0977, Val Acc: 1.0000\n",
            "Epoch 108/200, Train Loss: 0.0220, Val Loss: 1.1053, Val Acc: 0.7500\n",
            "Epoch 109/200, Train Loss: 0.0219, Val Loss: 1.1004, Val Acc: 0.7500\n",
            "Epoch 110/200, Train Loss: 0.0217, Val Loss: 1.0970, Val Acc: 0.7500\n",
            "Epoch 111/200, Train Loss: 0.0216, Val Loss: 1.1031, Val Acc: 0.7500\n",
            "Epoch 112/200, Train Loss: 0.0214, Val Loss: 1.0863, Val Acc: 1.0000\n",
            "Epoch 113/200, Train Loss: 0.0212, Val Loss: 1.1041, Val Acc: 0.7500\n",
            "Epoch 114/200, Train Loss: 0.0211, Val Loss: 1.0732, Val Acc: 1.0000\n",
            "Epoch 115/200, Train Loss: 0.0210, Val Loss: 1.1014, Val Acc: 0.7500\n",
            "Epoch 116/200, Train Loss: 0.0209, Val Loss: 1.0638, Val Acc: 1.0000\n",
            "Epoch 117/200, Train Loss: 0.0207, Val Loss: 1.1028, Val Acc: 0.7500\n",
            "Epoch 118/200, Train Loss: 0.0206, Val Loss: 1.0552, Val Acc: 1.0000\n",
            "Epoch 119/200, Train Loss: 0.0204, Val Loss: 1.0996, Val Acc: 0.7500\n",
            "Epoch 120/200, Train Loss: 0.0202, Val Loss: 1.0566, Val Acc: 1.0000\n",
            "Epoch 121/200, Train Loss: 0.0200, Val Loss: 1.0864, Val Acc: 0.7500\n",
            "Epoch 122/200, Train Loss: 0.0199, Val Loss: 1.0561, Val Acc: 1.0000\n",
            "Epoch 123/200, Train Loss: 0.0198, Val Loss: 1.0752, Val Acc: 0.7500\n",
            "Epoch 124/200, Train Loss: 0.0196, Val Loss: 1.0630, Val Acc: 0.7500\n",
            "Epoch 125/200, Train Loss: 0.0196, Val Loss: 1.0708, Val Acc: 0.7500\n",
            "Epoch 126/200, Train Loss: 0.0194, Val Loss: 1.0611, Val Acc: 0.7500\n",
            "Epoch 127/200, Train Loss: 0.0193, Val Loss: 1.0738, Val Acc: 0.7500\n",
            "Epoch 128/200, Train Loss: 0.0192, Val Loss: 1.0584, Val Acc: 0.7500\n",
            "Epoch 129/200, Train Loss: 0.0190, Val Loss: 1.0661, Val Acc: 1.0000\n",
            "Epoch 130/200, Train Loss: 0.0189, Val Loss: 1.0541, Val Acc: 0.7500\n",
            "Epoch 131/200, Train Loss: 0.0187, Val Loss: 1.0634, Val Acc: 1.0000\n",
            "Epoch 132/200, Train Loss: 0.0186, Val Loss: 1.0431, Val Acc: 1.0000\n",
            "Epoch 133/200, Train Loss: 0.0186, Val Loss: 1.0583, Val Acc: 1.0000\n",
            "Epoch 134/200, Train Loss: 0.0184, Val Loss: 1.0361, Val Acc: 1.0000\n",
            "Epoch 135/200, Train Loss: 0.0183, Val Loss: 1.0480, Val Acc: 1.0000\n",
            "Epoch 136/200, Train Loss: 0.0181, Val Loss: 1.0373, Val Acc: 1.0000\n",
            "Epoch 137/200, Train Loss: 0.0179, Val Loss: 1.0485, Val Acc: 1.0000\n",
            "Epoch 138/200, Train Loss: 0.0178, Val Loss: 1.0303, Val Acc: 1.0000\n",
            "Epoch 139/200, Train Loss: 0.0178, Val Loss: 1.0446, Val Acc: 1.0000\n",
            "Epoch 140/200, Train Loss: 0.0177, Val Loss: 1.0253, Val Acc: 1.0000\n",
            "Epoch 141/200, Train Loss: 0.0176, Val Loss: 1.0392, Val Acc: 1.0000\n",
            "Epoch 142/200, Train Loss: 0.0174, Val Loss: 1.0137, Val Acc: 1.0000\n",
            "Epoch 143/200, Train Loss: 0.0173, Val Loss: 1.0408, Val Acc: 0.7500\n",
            "Epoch 144/200, Train Loss: 0.0173, Val Loss: 1.0080, Val Acc: 1.0000\n",
            "Epoch 145/200, Train Loss: 0.0172, Val Loss: 1.0382, Val Acc: 0.7500\n",
            "Epoch 146/200, Train Loss: 0.0171, Val Loss: 1.0102, Val Acc: 1.0000\n",
            "Epoch 147/200, Train Loss: 0.0170, Val Loss: 1.0359, Val Acc: 0.7500\n",
            "Epoch 148/200, Train Loss: 0.0169, Val Loss: 1.0061, Val Acc: 1.0000\n",
            "Epoch 149/200, Train Loss: 0.0168, Val Loss: 1.0354, Val Acc: 0.7500\n",
            "Epoch 150/200, Train Loss: 0.0167, Val Loss: 1.0077, Val Acc: 1.0000\n",
            "Epoch 151/200, Train Loss: 0.0166, Val Loss: 1.0241, Val Acc: 1.0000\n",
            "Epoch 152/200, Train Loss: 0.0165, Val Loss: 1.0117, Val Acc: 1.0000\n",
            "Epoch 153/200, Train Loss: 0.0163, Val Loss: 1.0156, Val Acc: 1.0000\n",
            "Epoch 154/200, Train Loss: 0.0163, Val Loss: 1.0113, Val Acc: 1.0000\n",
            "Epoch 155/200, Train Loss: 0.0162, Val Loss: 1.0151, Val Acc: 1.0000\n",
            "Epoch 156/200, Train Loss: 0.0162, Val Loss: 1.0055, Val Acc: 1.0000\n",
            "Epoch 157/200, Train Loss: 0.0160, Val Loss: 1.0133, Val Acc: 1.0000\n",
            "Epoch 158/200, Train Loss: 0.0160, Val Loss: 1.0012, Val Acc: 1.0000\n",
            "Epoch 159/200, Train Loss: 0.0159, Val Loss: 1.0106, Val Acc: 1.0000\n",
            "Epoch 160/200, Train Loss: 0.0158, Val Loss: 0.9962, Val Acc: 1.0000\n",
            "Epoch 161/200, Train Loss: 0.0156, Val Loss: 1.0067, Val Acc: 1.0000\n",
            "Epoch 162/200, Train Loss: 0.0156, Val Loss: 0.9980, Val Acc: 1.0000\n",
            "Epoch 163/200, Train Loss: 0.0155, Val Loss: 1.0008, Val Acc: 1.0000\n",
            "Epoch 164/200, Train Loss: 0.0154, Val Loss: 0.9940, Val Acc: 1.0000\n",
            "Epoch 165/200, Train Loss: 0.0153, Val Loss: 0.9983, Val Acc: 1.0000\n",
            "Epoch 166/200, Train Loss: 0.0152, Val Loss: 0.9928, Val Acc: 1.0000\n",
            "Epoch 167/200, Train Loss: 0.0152, Val Loss: 0.9896, Val Acc: 1.0000\n",
            "Epoch 168/200, Train Loss: 0.0151, Val Loss: 0.9926, Val Acc: 1.0000\n",
            "Epoch 169/200, Train Loss: 0.0150, Val Loss: 0.9811, Val Acc: 1.0000\n",
            "Epoch 170/200, Train Loss: 0.0150, Val Loss: 0.9947, Val Acc: 1.0000\n",
            "Epoch 171/200, Train Loss: 0.0149, Val Loss: 0.9743, Val Acc: 1.0000\n",
            "Epoch 172/200, Train Loss: 0.0149, Val Loss: 0.9947, Val Acc: 1.0000\n",
            "Epoch 173/200, Train Loss: 0.0148, Val Loss: 0.9746, Val Acc: 1.0000\n",
            "Epoch 174/200, Train Loss: 0.0147, Val Loss: 0.9937, Val Acc: 1.0000\n",
            "Epoch 175/200, Train Loss: 0.0146, Val Loss: 0.9679, Val Acc: 1.0000\n",
            "Epoch 176/200, Train Loss: 0.0146, Val Loss: 0.9885, Val Acc: 1.0000\n",
            "Epoch 177/200, Train Loss: 0.0145, Val Loss: 0.9641, Val Acc: 1.0000\n",
            "Epoch 178/200, Train Loss: 0.0144, Val Loss: 0.9858, Val Acc: 1.0000\n",
            "Epoch 179/200, Train Loss: 0.0143, Val Loss: 0.9592, Val Acc: 1.0000\n",
            "Epoch 180/200, Train Loss: 0.0142, Val Loss: 0.9802, Val Acc: 1.0000\n",
            "Epoch 181/200, Train Loss: 0.0142, Val Loss: 0.9596, Val Acc: 1.0000\n",
            "Epoch 182/200, Train Loss: 0.0141, Val Loss: 0.9771, Val Acc: 1.0000\n",
            "Epoch 183/200, Train Loss: 0.0140, Val Loss: 0.9584, Val Acc: 1.0000\n",
            "Epoch 184/200, Train Loss: 0.0139, Val Loss: 0.9679, Val Acc: 1.0000\n",
            "Epoch 185/200, Train Loss: 0.0139, Val Loss: 0.9632, Val Acc: 1.0000\n",
            "Epoch 186/200, Train Loss: 0.0138, Val Loss: 0.9584, Val Acc: 1.0000\n",
            "Epoch 187/200, Train Loss: 0.0137, Val Loss: 0.9660, Val Acc: 1.0000\n",
            "Epoch 188/200, Train Loss: 0.0137, Val Loss: 0.9487, Val Acc: 1.0000\n",
            "Epoch 189/200, Train Loss: 0.0137, Val Loss: 0.9658, Val Acc: 1.0000\n",
            "Epoch 190/200, Train Loss: 0.0136, Val Loss: 0.9448, Val Acc: 1.0000\n",
            "Epoch 191/200, Train Loss: 0.0135, Val Loss: 0.9639, Val Acc: 1.0000\n",
            "Epoch 192/200, Train Loss: 0.0135, Val Loss: 0.9386, Val Acc: 1.0000\n",
            "Epoch 193/200, Train Loss: 0.0134, Val Loss: 0.9623, Val Acc: 1.0000\n",
            "Epoch 194/200, Train Loss: 0.0133, Val Loss: 0.9407, Val Acc: 1.0000\n",
            "Epoch 195/200, Train Loss: 0.0133, Val Loss: 0.9577, Val Acc: 1.0000\n",
            "Epoch 196/200, Train Loss: 0.0132, Val Loss: 0.9450, Val Acc: 1.0000\n",
            "Epoch 197/200, Train Loss: 0.0132, Val Loss: 0.9496, Val Acc: 1.0000\n",
            "Epoch 198/200, Train Loss: 0.0131, Val Loss: 0.9470, Val Acc: 1.0000\n",
            "Epoch 199/200, Train Loss: 0.0130, Val Loss: 0.9460, Val Acc: 1.0000\n",
            "Epoch 200/200, Train Loss: 0.0130, Val Loss: 0.9519, Val Acc: 1.0000\n"
          ]
        }
      ]
    }
  ]
}