{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1GDzZiuRMNSjALJMqerLLYX4tGKe89blo",
      "authorship_tag": "ABX9TyMGJsfI0YV9dwHJ2pixMfzA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thaianh1210/DeepLearning-exercise/blob/main/Untitled50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rbQzKK_IIKHh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POoxA-W5GTzh",
        "outputId": "23494a86-6c00-4fbe-bcbe-822029ac2d3c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = '/content/drive/MyDrive/img_cls_scenes_classification (1)/scenes_classification'\n",
        "train_dir = os.path.join(root_dir, 'train')\n",
        "test_dir = os.path.join(root_dir, 'val')\n",
        "classes = {\n",
        "    label_idx : class_name \\\n",
        "    for label_idx, class_name in enumerate(sorted(os.listdir(train_dir)))\n",
        "}\n",
        "print(classes)\n",
        "\n",
        "X_train, X_test = [], []\n",
        "y_train, y_test = [], []\n",
        "for dataset_path in [train_dir, test_dir]:\n",
        "  for label_idx, class_name in classes.items():\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    for img_filename in os.listdir(class_dir):\n",
        "      img_path = os.path.join(class_dir, img_filename)\n",
        "\n",
        "      if 'train' in dataset_path:\n",
        "        X_train.append(img_path)\n",
        "        y_train.append(label_idx)\n",
        "      else:\n",
        "        X_test.append(img_path)\n",
        "        y_test.append(label_idx)\n",
        "\n",
        "print(f'X_train: {len(X_train)}')\n",
        "print(f'X_test: {len(X_test)}')\n",
        "print(f'y_train: {len(y_train)}')\n",
        "print(f'y_test: {len(y_test)}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZMdaWK1Kpl6",
        "outputId": "a93d9382-1c57-4039-985d-23b6877706a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'buildings', 1: 'forest', 2: 'glacier', 3: 'mountain', 4: 'sea', 5: 'street'}\n",
            "X_train: 4468\n",
            "X_test: 3000\n",
            "y_train: 4468\n",
            "y_test: 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train test split\n",
        "seed = 0\n",
        "val_size = 0.3\n",
        "is_shuffle = True\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = val_size, random_state = seed, shuffle = is_shuffle)\n"
      ],
      "metadata": {
        "id": "Hn4DC46P3Ba2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SceneDataset(Dataset):\n",
        "  def __init__(self, X, y, transform = None):\n",
        "    self.img_paths = X\n",
        "    self.labels = y\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_paths)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.img_paths[idx]\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    return img, self.labels[idx]\n",
        "\n",
        "def transform(img, img_size = (224, 224)):\n",
        "  img = img.resize(img_size)\n",
        "  img = np.array(img)[..., :3]\n",
        "  img = torch.tensor(img).permute(2, 0, 1).float()\n",
        "  return img / 255.0"
      ],
      "metadata": {
        "id": "juWpRd-d35ky"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SceneDataset(X_train, y_train, transform = transform)\n",
        "val_dataset = SceneDataset(X_val, y_val, transform = transform)\n",
        "test_dataset = SceneDataset(X_test, y_test, transform = transform)\n",
        "\n",
        "train_batch_size = 64\n",
        "test_batch_size = 8\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = train_batch_size, shuffle = is_shuffle)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = test_batch_size, shuffle = False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size = test_batch_size, shuffle = is_shuffle)"
      ],
      "metadata": {
        "id": "t2YmIMlw5lVc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BottleNeckBlock(nn.Module):\n",
        "  def __init__(self, in_channels, growth_rate):\n",
        "    super(BottleNeckBlock, self).__init__()\n",
        "    self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "    self.conv1 = nn.Conv2d(in_channels, 4 * growth_rate, kernel_size = 1, bias = False)\n",
        "    self.bn2 = nn.BatchNorm2d(4* growth_rate)\n",
        "    self.conv2 = nn.Conv2d(4* growth_rate, growth_rate, kernel_size = 3, padding = 1, bias = False)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    res = x.clone().detach()\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = torch.cat([res, x], 1)\n",
        "    return x\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "  def __init__(self, num_layers, in_channels, growth_rate):\n",
        "    super(DenseBlock, self).__init__()\n",
        "    layers = []\n",
        "    for i in range(num_layers):\n",
        "      layers.append(BottleNeckBlock(in_channels + i* growth_rate, growth_rate))\n",
        "    self.block = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.block(x)"
      ],
      "metadata": {
        "id": "n4Q5QNGRISk9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseNet(nn.Module):\n",
        "  def __init__(self, num_blocks, growth_rate, num_classes):\n",
        "    super(DenseNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 2 * growth_rate, kernel_size = 7, stride = 2, padding = 3, bias = True)\n",
        "    self.bn1 = nn.BatchNorm2d(2* growth_rate)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "\n",
        "    self.dense_blocks = nn.ModuleList()\n",
        "    in_channels = 2* growth_rate\n",
        "    for i, num_layers in enumerate(num_blocks):\n",
        "      self.dense_blocks.append(DenseBlock(num_layers, in_channels, growth_rate))\n",
        "      in_channels += num_layers * growth_rate\n",
        "      if i != len(num_blocks) - 1:\n",
        "        out_channels = in_channels // 2\n",
        "        self.dense_blocks.append(nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size = 1, bias = False),\n",
        "            nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
        "        )\n",
        "        )\n",
        "        in_channels = out_channels\n",
        "    self.bn2 = nn.BatchNorm2d(in_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool2 = nn.AvgPool2d(kernel_size = 7)\n",
        "    self.fc = nn.Linear(in_channels, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    for block in self.dense_blocks:\n",
        "      x = block(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "jPn2PVfVSIWg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(list(classes.keys()))\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = DenseNet([6, 12, 24, 16], growth_rate = 32, num_classes = n_classes).to(device)"
      ],
      "metadata": {
        "id": "Jg_30a5BbGOx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_dataloader, criterion, device):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  losses = []\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_dataloader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      losses.append(loss.item())\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  val_acc = correct / total\n",
        "  val_loss = sum(losses) / len(losses)\n",
        "  return val_acc, val_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "HtVwttridpbx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, train_dataloader, criterion, optimizer, num_epochs, device):\n",
        "  train_losses = []\n",
        "  train_accuracies = []\n",
        "  for epoch in range(num_epochs):\n",
        "    batch_train_losses = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.train()\n",
        "    for idx, (inputs, labels) in enumerate(train_dataloader):\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      batch_train_losses.append(loss.item())\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "    acc = correct / total\n",
        "    train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(acc)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {acc:.4f}')\n",
        "  return train_losses, train_accuracies\n"
      ],
      "metadata": {
        "id": "C9QBRd3xeyxj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
        "num_epochs = 50\n",
        "train_losses, train_accuracies = fit(model, train_dataloader, criterion, optimizer, num_epochs, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8LGleZUgtjl",
        "outputId": "c1382c26-4698-419c-be89-fe18a112ee70"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 0.4785, Train Acc: 0.8438\n",
            "Epoch 2/50, Train Loss: 0.4554, Train Acc: 0.8509\n",
            "Epoch 3/50, Train Loss: 0.4421, Train Acc: 0.8536\n",
            "Epoch 4/50, Train Loss: 0.4269, Train Acc: 0.8597\n",
            "Epoch 5/50, Train Loss: 0.4158, Train Acc: 0.8583\n",
            "Epoch 6/50, Train Loss: 0.4061, Train Acc: 0.8597\n",
            "Epoch 7/50, Train Loss: 0.3981, Train Acc: 0.8650\n",
            "Epoch 8/50, Train Loss: 0.3920, Train Acc: 0.8583\n",
            "Epoch 9/50, Train Loss: 0.3828, Train Acc: 0.8655\n",
            "Epoch 10/50, Train Loss: 0.3749, Train Acc: 0.8666\n",
            "Epoch 11/50, Train Loss: 0.3723, Train Acc: 0.8711\n",
            "Epoch 12/50, Train Loss: 0.3685, Train Acc: 0.8695\n",
            "Epoch 13/50, Train Loss: 0.3601, Train Acc: 0.8740\n",
            "Epoch 14/50, Train Loss: 0.3579, Train Acc: 0.8693\n",
            "Epoch 15/50, Train Loss: 0.3492, Train Acc: 0.8744\n",
            "Epoch 16/50, Train Loss: 0.3458, Train Acc: 0.8767\n",
            "Epoch 17/50, Train Loss: 0.3402, Train Acc: 0.8798\n",
            "Epoch 18/50, Train Loss: 0.3365, Train Acc: 0.8778\n",
            "Epoch 19/50, Train Loss: 0.3340, Train Acc: 0.8787\n",
            "Epoch 20/50, Train Loss: 0.3331, Train Acc: 0.8847\n",
            "Epoch 21/50, Train Loss: 0.3270, Train Acc: 0.8823\n",
            "Epoch 22/50, Train Loss: 0.3251, Train Acc: 0.8818\n",
            "Epoch 23/50, Train Loss: 0.3189, Train Acc: 0.8859\n",
            "Epoch 24/50, Train Loss: 0.3187, Train Acc: 0.8825\n",
            "Epoch 25/50, Train Loss: 0.3168, Train Acc: 0.8854\n",
            "Epoch 26/50, Train Loss: 0.3141, Train Acc: 0.8868\n",
            "Epoch 27/50, Train Loss: 0.3071, Train Acc: 0.8901\n",
            "Epoch 28/50, Train Loss: 0.3047, Train Acc: 0.8932\n",
            "Epoch 29/50, Train Loss: 0.3009, Train Acc: 0.8894\n",
            "Epoch 30/50, Train Loss: 0.2971, Train Acc: 0.8935\n",
            "Epoch 31/50, Train Loss: 0.2989, Train Acc: 0.8892\n",
            "Epoch 32/50, Train Loss: 0.2971, Train Acc: 0.8876\n",
            "Epoch 33/50, Train Loss: 0.2922, Train Acc: 0.8948\n",
            "Epoch 34/50, Train Loss: 0.2874, Train Acc: 0.8939\n",
            "Epoch 35/50, Train Loss: 0.2884, Train Acc: 0.8959\n",
            "Epoch 36/50, Train Loss: 0.2823, Train Acc: 0.8997\n",
            "Epoch 37/50, Train Loss: 0.2794, Train Acc: 0.9000\n",
            "Epoch 38/50, Train Loss: 0.2792, Train Acc: 0.8946\n",
            "Epoch 39/50, Train Loss: 0.2772, Train Acc: 0.8997\n",
            "Epoch 40/50, Train Loss: 0.2739, Train Acc: 0.9040\n",
            "Epoch 41/50, Train Loss: 0.2689, Train Acc: 0.9013\n",
            "Epoch 42/50, Train Loss: 0.2689, Train Acc: 0.9022\n",
            "Epoch 43/50, Train Loss: 0.2673, Train Acc: 0.9062\n",
            "Epoch 44/50, Train Loss: 0.2626, Train Acc: 0.9064\n",
            "Epoch 45/50, Train Loss: 0.2659, Train Acc: 0.9058\n",
            "Epoch 46/50, Train Loss: 0.2590, Train Acc: 0.9089\n",
            "Epoch 47/50, Train Loss: 0.2559, Train Acc: 0.9109\n",
            "Epoch 48/50, Train Loss: 0.2540, Train Acc: 0.9100\n",
            "Epoch 49/50, Train Loss: 0.2511, Train Acc: 0.9107\n",
            "Epoch 50/50, Train Loss: 0.2464, Train Acc: 0.9143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_accuracies, val_losses = evaluate(model, val_dataloader, criterion, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "p_stXG1-yzlM",
        "outputId": "1164582d-5915-4809-db5c-2ef787a7902f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-aa0da06aeab8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-9bc4b7fddc78>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, test_dataloader, criterion, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-a727c3172306>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}